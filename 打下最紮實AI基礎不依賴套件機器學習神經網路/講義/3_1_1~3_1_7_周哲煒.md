## 3.1.1~3.1.7_周哲煒  
[Toc]

### 3.1.1 text sample data I/O  

```python=
x,y=[],[]
with open('food_truck_data.txt') as data_f:  #file IO
    content = data_f.readlines()  #read file content, to a list
    for line in content:
        tmp = line.split(",")
        x.append(tmp[0])
        y.append(tmp[1])

```
---
### 3.1.2 What is machine learning do?  

```
機器學習的目標:找出函數(eg: y = f(x) = ax+b)描述資料的關係
x:=樣本特徵
y:=樣本標籤
求解的過程-->訓練模型(找出預測值和目標值的最小誤差)
```


#### 機器學習主要分三類  
  
  1. 監督式學習: 用答案去訓練  
        - 需要訓練樣本  
        - 設計可以 **良好** 刻劃sample data <--> sample label關係的函數  
        - 選擇**合理的**損失函數，描述誤差  
        - 訓練模型  
        - 使用模型  
    p.s.分類問題、回歸問題  
  2. 非監督式學習:自己找規律猜答案  
  3. 強化學習:與環境互動的經驗回饋來訓練  

---
### 3.1.3線性回歸  
![](https://hackmd.io/_uploads/BJBHpfC23.jpg)


---
### 3.1.4正規方程式法求解(一堆矩陣數學)  
![](https://hackmd.io/_uploads/HyvQ0fAn2.jpg)
![](https://hackmd.io/_uploads/Sy9jRzA2n.jpg)
##### *正規方程式法求解餐車利潤問題*  
```python=
import numpy

#data是mx2的矩陣，每行表示一個樣本
data = np.loadtxt('food_truck_data.txt',delimeter=',')
train_x = data[:,0] #城市人口，mx1的矩陣
train_y = data[:,1] #餐車利潤，mx1的矩陣

X = np.ones(shape=len(train_x), 2)
X[:,1] = train_x
y = train_y

XT = X.transpose()
XTy = XT@y

w = np.linalg.inv(XT@X)@ XTy
print(w)
```
---
### 3.1.5梯度下降法加速求解  

>正規方法需要計算矩陣乘法&反矩陣-->若特徵或樣本較多，則太耗時。  
>一般用梯度下降法來求解(較快速)  

從(w_0,b_0)出發，透過下列公式更新  

$$w_{i+1}:=w_i- \alpha \frac{\partial L}{\partial w_i}$$  

$$b_{i+1}:=w_i- \alpha \frac{\partial L}{\partial b
_i}$$ 

其中  
$$\frac {\partial L}{\partial w_i}=np.mean((w_ix+b_i-y)\cdot x)$$
$$\frac {\partial L}{\partial b_i}=np.mean(w_ix+b_i-y)$$

**觀察疊代情況-->繪製*loss curve***

---
### 3.1.6偵錯學習率

> 偵錯學習率的過程就是用不同的學習率嘗試學習  

+ 一樣是用*loss curve*來觀察  

---
### 3.1.7梯度驗證  
>在執行梯度下降法前，應進行梯度驗證，以保證梯度和函數值的計算正確

對線性回歸問題，應使用以下數值梯度公式來檢驗:  
(數值梯度)  
$$\frac{\partial L(w,b)}{\partial w}=lim_{\epsilon ->0} \frac{L(w+\epsilon,b)-L(w-\epsilon ,b)}{2\epsilon}$$
$$\frac{\partial L(w,b)}{\partial b}=lim_{\epsilon ->0} \frac{L(w,b+\epsilon)-L(w,b-\epsilon)}{2\epsilon}$$

#### code
分析梯度  
```python=
import numpy as np

dw = np.mean((w*x+b-y)*x)
db = np.mean(w*x+b-y)
```
數值梯度  
```python=
df_approx = lambdax,y,w,b,eps:((loss(x,y,w+eps,b)\
-loss(x,y,w-eps,b))/(2*eps),(loss(x,y,w,b+eps)\
-loss(x,y,w,b-eps))/(2*eps))
```
在任意點，如(w,b)=(1.0,-2.0)，比較數值梯度和分析梯度，若非常接近則可以使用  

```python=
import numpy

w,b,eps = 1.0,-2.0,1e-8
dw = np.mean((w*X+b-y)*X)
db = np.mean(w*X+b-y)
grad = np.array([dw,db])
grad_approx = df_approx(X,y,w,b,eps)
print(grad)
print(grad_approx)

```