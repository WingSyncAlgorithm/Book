# -*- coding: utf-8 -*-
"""邏輯回歸(numpy)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cJxq0-ZN8cbcYSp_C9lgbGHGQxwlvZvS
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
# 引入必要的庫，並設置在Notebook中繪圖

# 固定隨機數據生成
np.random.seed(0)
# 定義數據點總數
n_pts = 100
# 定義特徵維度
D = 2

# 從正態分佈生成數據點，兩類數據
Xa = np.array([np.random.normal(10, 2, n_pts),
               np.random.normal(12, 2, n_pts)])
Xb = np.array([np.random.normal(5, 2, n_pts),
               np.random.normal(6, 2, n_pts)])

# 將兩類特徵矩陣串聯在一起，並轉置
X = np.append(Xa, Xb, axis=1).T
# 生成對應的標籤，前半部分為0，後半部分為1
Y = (np.append(np.zeros(n_pts), np.ones(n_pts))).T

print(X[::50])
print(Y[::50])

fig, ax = plt.subplots(figsize=(4, 4))
ax.scatter(X[:n_pts, 0], X[:n_pts, 1], color='lightcoral', label='$Y = 0$')
ax.scatter(X[n_pts:, 0], X[n_pts:, 1], color='blue', label='$Y = 1$')
ax.set_title('Sample Dataset')
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.legend(loc=4)
plt.show()

def sigmoid(z):
    """對輸入數組 z 逐元素應用 sigmoid 函數。"""
    return 1 / (1 + np.exp(-z))

def gradient_descent_logistic_reg(X, y, lambda_, alpha, num_iters, gamma=0.8, epsilon=1e-8):
    # 初始化變數
    w_history = []

    # 添加特徵 1 到特徵矩陣 X 中
    X = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))

    num_features = X.shape[1]
    v = np.zeros_like(num_features)
    w = np.zeros(num_features)

    for n in range(num_iters):
        predictions = sigmoid(X @ w)
        errors = predictions - y

        # 計算梯度，同時加入正則化項
        gradient = (errors.T @ X) / len(y) + 2 * lambda_ * w

        if np.max(np.abs(gradient)) < epsilon:
            print("梯度足夠小！")
            print("迭代次數為：", n)
            break

        # 更新梯度方向，使用 Momentum 技巧
        v = gamma * v + alpha * gradient
        w = w - v

        # 計算成本（損失）函數
        cost = - np.mean(np.log(predictions) * y + np.log(1 - predictions).T * (1 - y)) + lambda_ * np.sum(w**2)
        w_history.append(w)

    return w_history
def loss_logistic(W, X, y, reg=0.):
    f = sigmoid(X @ W[1:] + W[0])
    loss = -np.mean((y * np.log(f)) + ((1 - y) * np.log(1 - f)))
    loss += reg * (np.sum(np.square(W)))
    return loss
def loss_history_logistic(w_history, X, y, reg=0.):
    loss_history = []
    for w in w_history:
        loss_history.append(loss_logistic(w, X, y, reg))
    return loss_history
reg = 0.0
alpha = 0.01
iterations = 10000
w_history = gradient_descent_logistic_reg(X, Y, reg, alpha, iterations)
w = w_history[-1]
print("w:", w)
loss_history = loss_history_logistic(w_history, X, Y, reg)
print(loss_history[:-1:len(loss_history)//10])

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))

# 計算決策邊界的兩點，以繪製決策曲線
x1 = np.array([X[:, 0].min()-1, X[:, 0].max()+1])
x2 = -w[0] / w[2] + x1 * (-w[1] / w[2])

# 在第一個子圖中繪製散點圖和決策曲線
ax[0].plot(x1, x2, color='k', linestyle='--', linewidth=2)
ax[0].scatter(X[:n_pts, 0], X[:n_pts, 1], color='lightcoral', label='$y = 0$')
ax[0].scatter(X[n_pts:, 0], X[n_pts:, 1], color='blue', label='$y = 1$')
ax[0].set_title('$x_1$ vs. $x_2$')
ax[0].set_xlabel('$x_1$')
ax[0].set_ylabel('$x_2$')
ax[0].legend(loc=4)

# 在第二個子圖中繪製損失函數的變化
ax[1].plot(loss_history, color='r')
ax[1].set_ylim(0, ax[1].get_ylim()[1])
ax[1].set_title(r'$J(w)$ vs. Iteration')
ax[1].set_xlabel('Iteration')
ax[1].set_ylabel(r'$J(w)$')

# 調整子圖佈局
fig.tight_layout()

# 顯示圖形
plt.show()

# Print accuracy
X_1 = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))  # 添加一列特徵"1"，即偏置項
Y_predictions = sigmoid(X_1 @ w) >= 0.5  # 預測標籤
accuracy = float((np.dot(Y, Y_predictions) + np.dot(1 - Y, 1 - Y_predictions)) / float(Y.size) * 100)
print("預測的準確性是：{}%".format(accuracy))