# -*- coding: utf-8 -*-
"""鳶尾花分類(numpy)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ScRLU1qnbwQPdBQ0Tg8SDKykwPdQpYhe
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# 使用 pandas 讀取 "iris.csv" 檔案
iris = pd.read_csv("iris.csv")

# 打亂資料集的行順序，以隨機排列的方式
shuffled_rows = np.random.permutation(iris.index)
iris = iris.loc[shuffled_rows, :]

# 輸出前幾筆資料，確認數據讀取和打亂是否正確
print(iris.head())

# 輸出所有不重複的鳶尾花物種
print(iris.variety.unique())

# 繪製數據的直方圖
iris.hist()

# 顯示繪製的圖形
plt.show()

X = iris[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values

# 將 Iris-versicolor 類別的標籤設定為 1，將 Iris-virginica 類別的標籤設定為 0
Y = (iris.variety == 'Iris-versicolor').values.astype(int)

# 輸出部分特徵矩陣 X 的前 3 列
print(X[:3])

# 輸出部分目標向量 y 的前 3 值
print(y[:3])
def sigmoid(z):
    """對輸入數組 z 逐元素應用 sigmoid 函數。"""
    return 1 / (1 + np.exp(-z))

def gradient_descent_logistic_reg(X, y, lambda_, alpha, num_iters, gamma=0.8, epsilon=1e-8):
    # 初始化變數
    w_history = []

    # 添加特徵 1 到特徵矩陣 X 中
    X = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))

    num_features = X.shape[1]
    v = np.zeros_like(num_features)
    w = np.zeros(num_features)

    for n in range(num_iters):
        predictions = sigmoid(X @ w)
        errors = predictions - y

        # 計算梯度，同時加入正則化項
        gradient = (errors.T @ X) / len(y) + 2 * lambda_ * w

        if np.max(np.abs(gradient)) < epsilon:
            print("梯度足夠小！")
            print("迭代次數為：", n)
            break

        # 更新梯度方向，使用 Momentum 技巧
        v = gamma * v + alpha * gradient
        w = w - v

        # 計算成本（損失）函數
        cost = - np.mean(np.log(predictions) * y + np.log(1 - predictions).T * (1 - y)) + lambda_ * np.sum(w**2)
        w_history.append(w)

    return w_history
def loss_logistic(W, X, y, reg=0.):
    f = sigmoid(X @ W[1:] + W[0])
    loss = -np.mean((y * np.log(f)) + ((1 - y) * np.log(1 - f)))
    loss += reg * (np.sum(np.square(W)))
    return loss
def loss_history_logistic(w_history, X, y, reg=0.):
    loss_history = []
    for w in w_history:
        loss_history.append(loss_logistic(w, X, y, reg))
    return loss_history
reg = 0.0
alpha = 0.0001
iterations = 10000

# 執行梯度下降法求解模型參數
w_history = gradient_descent_logistic_reg(X, Y, reg, alpha, iterations)
w = w_history[-1]
print("w:", w)

# 計算損失函數的變化
loss_history = loss_history_logistic(w_history, X, Y, reg)
print(loss_history[:-1:len(loss_history)//10])

# 添加特徵 1 到特徵矩陣 X 中
X_1 = np.hstack((np.ones((X.shape[0], 1), dtype=X.dtype), X))

# 用訓練好的模型進行預測
y_predictions = sigmoid(X_1 @ w) >= 0.5

# 計算並輸出準確性
accuracy = float((np.dot(Y, y_predictions) + np.dot(1 - Y, 1 - y_predictions)) / float(Y.size) * 100)
print("預測的準確性是：", accuracy, "%")

# 繪製損失函數的變化
plt.plot(loss_history, color='r')
plt.title(r'$J(w)$ vs. Iteration')
plt.xlabel('Iteration')
plt.ylabel(r'$J(w)$')
plt.show()

